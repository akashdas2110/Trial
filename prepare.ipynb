{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "QSa4_IBmK19f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_uGb9GTvrQWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0712c9dc-43e4-4011-8a80-331726212ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google drive"
      ],
      "metadata": {
        "id": "dWp8BIJuK6WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03bcbea-0df2-4779-ec33-f393a2f8c7cb",
        "id": "NzQR2mDbiLo8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Necessary Functions"
      ],
      "metadata": {
        "id": "WPa8uLrTLAXe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gPYfeYaprQWj"
      },
      "outputs": [],
      "source": [
        "# Function to load data from a given file path\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    return data\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    # Lowercasing\n",
        "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "    # Tokenization\n",
        "    data['text'] = data['text'].apply(lambda x: word_tokenize(x))\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    data['text'] = data['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    data['text'] = data['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "    # Joining tokens back to string\n",
        "    data['text'] = data['text'].apply(lambda x: ' '.join(x))\n",
        "    return data\n",
        "\n",
        "# Function to split the data into train/validation/test sets\n",
        "def split_data(data, test_size=0.2, val_size=0.25):\n",
        "    # Splitting data into train and temp (temp will be further split into validation and test)\n",
        "    train_data, temp_data = train_test_split(data, test_size=test_size, random_state=42)\n",
        "    # Splitting temp_data into validation and test\n",
        "    validation_data, test_data = train_test_split(temp_data, test_size=val_size, random_state=42)\n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "# Function to store the splits at train.csv/validation.csv/test.csv\n",
        "def store_splits(train_data, validation_data, test_data, output_path):\n",
        "    train_data.to_csv(output_path + 'train.csv', index=False)\n",
        "    validation_data.to_csv(output_path + 'validation.csv', index=False)\n",
        "    test_data.to_csv(output_path + 'test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "GCI47QVhL-C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Applied_ML_A1/emails.csv'  # Update with the actual path\n",
        "data = load_data(file_path)"
      ],
      "metadata": {
        "id": "wj5USMyCwYFA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Preprocess data"
      ],
      "metadata": {
        "id": "Tv9tb9W6MC-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data= preprocess_data(data)"
      ],
      "metadata": {
        "id": "lkIEfXQ7weUV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Split data"
      ],
      "metadata": {
        "id": "rhoPGukFMHAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data, test_data = split_data(processed_data, test_size=0.2, val_size=0.25)\n"
      ],
      "metadata": {
        "id": "4AI2UUY_wYFB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store Splited Data"
      ],
      "metadata": {
        "id": "bfYopydCMIlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_splits(train_data, validation_data, test_data, output_path='/content/drive/MyDrive/Applied_ML_A1/')"
      ],
      "metadata": {
        "id": "ZB89-U1exmzJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJk2Km1vxtpv"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}